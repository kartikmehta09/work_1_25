{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a Python solution using PyMuPDF and LangChain for PDF processing and metadata extraction. This code handles complex document structures and integrates with vector databases:\n",
    "\n",
    "# ```python\n",
    "import fitz  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def extract_pdf_structure(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    document_metadata = []\n",
    "    \n",
    "    for page_num, page in enumerate(doc):\n",
    "        page_data = {\n",
    "            \"page_number\": page_num + 1,\n",
    "            \"headers\": [],\n",
    "            \"subheaders\": [],\n",
    "            \"paragraphs\": [],\n",
    "            \"full_text\": \"\"\n",
    "        }\n",
    "        \n",
    "        # Extract text with formatting information\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        current_header = None\n",
    "        current_subheader = None\n",
    "        \n",
    "        for block in blocks:\n",
    "            if \"lines\" in block:\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        text = span[\"text\"].strip()\n",
    "                        font_size = span[\"size\"]\n",
    "                        \n",
    "                        # Header detection logic\n",
    "                        if font_size >= 20 and text:\n",
    "                            page_data[\"headers\"].append(text)\n",
    "                            current_header = text\n",
    "                            current_subheader = None\n",
    "                        elif font_size >= 16 and text:\n",
    "                            page_data[\"subheaders\"].append(text)\n",
    "                            current_subheader = text\n",
    "                        else:\n",
    "                            if text:\n",
    "                                paragraph_data = {\n",
    "                                    \"text\": text,\n",
    "                                    \"header\": current_header,\n",
    "                                    \"subheader\": current_subheader,\n",
    "                                    \"page\": page_num + 1\n",
    "                                }\n",
    "                                page_data[\"paragraphs\"].append(paragraph_data)\n",
    "                                page_data[\"full_text\"] += text + \"\\n\"\n",
    "        \n",
    "        document_metadata.append(page_data)\n",
    "    \n",
    "    return document_metadata\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    # Extract document structure\n",
    "    structured_data = extract_pdf_structure(pdf_path)\n",
    "    \n",
    "    # Recursive chunking with metadata preservation\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    for page in structured_data:\n",
    "        for paragraph in page[\"paragraphs\"]:\n",
    "            metadata = {\n",
    "                \"page\": page[\"page_number\"],\n",
    "                \"header\": paragraph[\"header\"],\n",
    "                \"subheader\": paragraph[\"subheader\"]\n",
    "            }\n",
    "            documents.append((paragraph[\"text\"], metadata))\n",
    "    \n",
    "    # Create FAISS vector store\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    texts = [doc[0] for doc in documents]\n",
    "    metadatas = [doc[1] for doc in documents]\n",
    "    \n",
    "    vector_db = FAISS.from_texts(texts, embeddings, metadatas=metadatas)\n",
    "    vector_db.save_local(\"pdf_vector_index\")\n",
    "    \n",
    "    return structured_data, vector_db\n",
    "\n",
    "# Usage\n",
    "pdf_path = \"your_document.pdf\"\n",
    "structured_data, vector_db = process_pdf(pdf_path)\n",
    "\n",
    "# Generate document structure report\n",
    "print(\"Document Structure Overview:\")\n",
    "for page in structured_data:\n",
    "    print(f\"\\nPage {page['page_number']}:\")\n",
    "    print(f\"Headers: {', '.join(page['headers'])}\")\n",
    "    print(f\"Subheaders: {', '.join(page['subheaders'])}\")\n",
    "\n",
    "# Key components of this solution:\n",
    "\n",
    "# **Document Structure Extraction**  \n",
    "# 1. **Header Detection**: Uses font size thresholds (adjust based on your document)\n",
    "# 2. **Metadata Tracking**: Maintains header/subheader context through document flow\n",
    "# 3. **Paragraph Association**: Links content to current header hierarchy\n",
    "\n",
    "# **Output Structure**  \n",
    "# The code generates:\n",
    "# 1. List of pages with headers/subheaders\n",
    "# 2. Vector database with chunked content and metadata\n",
    "# 3. Hierarchical document structure report\n",
    "\n",
    "# Example output format for document structure:\n",
    "# ```\n",
    "# Page 1:\n",
    "# Headers: Introduction, Abstract\n",
    "# Subheaders: Project Overview, Methodology\n",
    "\n",
    "# Page 2:\n",
    "# Headers: System Architecture\n",
    "# Subheaders: Components Diagram, Data Flow\n",
    "# ```\n",
    "\n",
    "# **Integration Options**  \n",
    "# For Chroma DB instead of FAISS:\n",
    "# ```python\n",
    "# from langchain.vectorstores import Chroma\n",
    "\n",
    "# # Replace FAISS code with:\n",
    "# vector_db = Chroma.from_texts(\n",
    "#     texts=texts,\n",
    "#     embedding=embeddings,\n",
    "#     metadatas=metadatas,\n",
    "#     persist_directory=\"chroma_db\"\n",
    "# )\n",
    "# ```\n",
    "\n",
    "# **Optimization Tips**:\n",
    "# 1. Adjust font size thresholds based on your document's typography\n",
    "# 2. Add logic for bold/italic text detection using `span[\"flags\"]`\n",
    "# 3. Implement custom chunking strategies for different content types\n",
    "# 4. Add error handling for malformed PDFs\n",
    "# 5. Include PDF text cleanup/normalization steps\n",
    "\n",
    "# This implementation provides a balance between automated structure detection and customizable parameters for handling various document formats.\n",
    "\n",
    "# ---\n",
    "# Answer from Perplexity: pplx.ai/share"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
