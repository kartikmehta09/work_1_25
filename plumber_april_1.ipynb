{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def process_page(page):\n",
    "    \"\"\"Extract structure from a single PDF page\"\"\"\n",
    "    structure = {\n",
    "        \"page_number\": page.page_number,\n",
    "        \"headers\": [],\n",
    "        \"subheaders\": [],\n",
    "        \"content\": [],\n",
    "        \"current_header\": None,\n",
    "        \"current_subheader\": None\n",
    "    }\n",
    "    \n",
    "    # Extract text elements with formatting info\n",
    "    elements = page.extract_text_lines(extra_attrs=[\"size\", \"fontname\"])\n",
    "    \n",
    "    for element in elements:\n",
    "        text = element[\"text\"].strip()\n",
    "        font_size = element[\"size\"]\n",
    "        \n",
    "        # Header detection (adjust thresholds based on your document)\n",
    "        if font_size >= 14 and text:\n",
    "            structure[\"headers\"].append(text)\n",
    "            structure[\"current_header\"] = text\n",
    "            structure[\"current_subheader\"] = None\n",
    "        elif font_size >= 12 and text:\n",
    "            structure[\"subheaders\"].append(text)\n",
    "            structure[\"current_subheader\"] = text\n",
    "        elif text:\n",
    "            structure[\"content\"].append({\n",
    "                \"text\": text,\n",
    "                \"header\": structure[\"current_header\"],\n",
    "                \"subheader\": structure[\"current_subheader\"]\n",
    "            })\n",
    "    \n",
    "    return structure\n",
    "\n",
    "def process_document(pdf_path):\n",
    "    \"\"\"Process entire PDF document\"\"\"\n",
    "    document = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            document.append(process_page(page))\n",
    "    return document\n",
    "\n",
    "def build_toc(document_data):\n",
    "    \"\"\"Generate table of contents\"\"\"\n",
    "    toc = []\n",
    "    for page in document_data:\n",
    "        for header in page[\"headers\"]:\n",
    "            toc.append({\"title\": header, \"page\": page[\"page_number\"], \"type\": \"header\"})\n",
    "        for subheader in page[\"subheaders\"]:\n",
    "            toc.append({\"title\": subheader, \"page\": page[\"page_number\"], \"type\": \"subheader\"})\n",
    "    return toc\n",
    "\n",
    "def prepare_chunks(document_data):\n",
    "    \"\"\"Create chunks with metadata\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    chunks = []\n",
    "    for page in document_data:\n",
    "        full_text = \" \".join([item[\"text\"] for item in page[\"content\"]])\n",
    "        page_chunks = text_splitter.split_text(full_text)\n",
    "        \n",
    "        for chunk in page_chunks:\n",
    "            chunks.append({\n",
    "                \"text\": chunk,\n",
    "                \"metadata\": {\n",
    "                    \"page\": page[\"page_number\"],\n",
    "                    \"header\": page[\"current_header\"],\n",
    "                    \"subheader\": page[\"current_subheader\"]\n",
    "                }\n",
    "            })\n",
    "    return chunks\n",
    "\n",
    "def save_to_faiss(chunks):\n",
    "    \"\"\"Store chunks in FAISS vector database\"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    metadatas = [chunk[\"metadata\"] for chunk in chunks]\n",
    "    \n",
    "    vector_store = FAISS.from_texts(texts, embeddings, metadatas=metadatas)\n",
    "    vector_store.save_local(\"faiss_index\")\n",
    "    return vector_store\n",
    "\n",
    "# Main execution flow\n",
    "def process_pdf(pdf_path):\n",
    "    document_data = process_document(pdf_path)\n",
    "    toc = build_toc(document_data)\n",
    "    chunks = prepare_chunks(document_data)\n",
    "    vector_db = save_to_faiss(chunks)\n",
    "    \n",
    "    # Print formatted table of contents\n",
    "    print(\"Table of Contents:\")\n",
    "    print(\"{:<50} {:<10}\".format(\"Title\", \"Page\"))\n",
    "    print(\"-\" * 60)\n",
    "    for entry in toc:\n",
    "        print(\"{:<50} {:<10}\".format(entry[\"title\"], entry[\"page\"]))\n",
    "    \n",
    "    return vector_db"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
